#!/usr/bin/env python

import os
os.environ["OPENBLAS_NUM_THREADS"] = "1"

'''
Statistical analysis of avalanches generated from time series
'''

# LIBRARIES


import sys
import numpy as np


################################################################################
################################################################################
################################################################################

'''
Functions for the analysis of time series.
'''

# 1: FUNCTIONS TO BUILD CLUSTERS.

# Takes as input a time series of interevent times and a value of the
# clustering parameter Delta. Returns a dictionary with the statistics
# of each cluster generated by the time series: size, duration
def clusters_from_TS(event, delta):


    s, t = 1, 0
    stats = {'S' : [], 'T' : []}

    for dt in event[1:-1]:
        if dt > delta:
            stats['S'].append(s)
            stats['T'].append(t)
            s, t = 0, 0
        else:
            t += dt
        s += 1

    stats['S'].append(s)
    stats['T'].append(t)
        
    out = prepare_output(stats, delta)

    return out



# Remove largest cluster and its duration from data
def prepare_output(stats, delta):

    if stats['S']:

        Sm = max(stats['S'])
        whereS = stats['S'].index(Sm)
        stats['Sm'] = [Sm]

        stats['S'].pop(whereS)
        stats['T'].pop(whereS)

        if stats['S']:
            return stats
        else:
            return None

    else:
        return None


################################################################################

'''
Functions for the statistical analysis of a generic array of data.
'''


# Compute probability distribution with log-spaced bins
def log_PDF(data, bins, int_data):

    eps = 1.05

    N = len(data)
    data_max, data_min = max(data), min(data)
    if data_min == 0:
        data = np.array(data)
        data_min = min(data[data > 0])



    d_px = {}
    d_x = {}
    num = {}
    norm = {}

    l_min = np.log(data_min)
    l_max = np.log(data_max) + eps
    l_dx = (l_max - l_min) / bins



    for n in data:

        if n > 0:

            k = int( ( np.log(n) - l_min) / l_dx)

            if k not in num:
                num[k] = 0.0
                d_x[k] = 0.0
                d_px[k] = 0.0
                p1 = np.exp(l_min + k * l_dx)
                p2 = np.exp(l_min + (k+1) * l_dx)
                if int_data:
                    p1 = int(p1)
                    p2 = int(p2)
                    if p1 == p2:
                        norm[k] = 1
                    else:
                        norm[k] = p2 - p1
                else:
                    norm[k] = p2 - p1

            d_px[k] += 1.0
            d_x[k] += n
            num[k] += 1.0


    for k in num:
        d_x[k] /= num[k]
        d_px[k] /= N
        d_px[k] /= norm[k]


    x = []
    px = []
    for k in range(0, bins):
        if k in num:
            if num[k] > 0:
                x.append(d_x[k])
                px.append(d_px[k])


    return np.array(x), np.array(px)


# Compute average value of data_y as a function of data_x using log-spaced bins
def log_average (data, data_y, bins, int_data):

    eps = 1.05

    data_max, data_min = max(data), min(data)
    if data_min == 0:
        data = np.array(data)
        data_min = min(data[data > 0])


    d_px = {}
    d_x = {}
    num = {}
    norm = {}


    l_min = np.log(data_min) 
    l_max = np.log(data_max) + eps
    l_dx = (l_max - l_min) / bins



    for i in range(0, len(data)):

        n = data[i]

        if n >= data_min:

            k = int( ( np.log(n) - l_min) / l_dx)

            if k not in num:
                num[k] = 0.0
                d_x[k] = 0.0
                d_px[k] = 0.0
                p1 = np.exp(l_min + k * l_dx)
                p2 = np.exp(l_min + (k+1) * l_dx)
                if int_data:
                    p1 = int(p1)
                    p2 = int(p2)
                    if p1 == p2:
                        norm[k] = 1
                    else:
                        norm[k] = p2 - p1
                else:
                    norm[k] = p2 - p1

            d_px[k] += data_y[i]
            d_x[k] += n
            num[k] += 1.0


    for k in num:
        d_x[k] /= num[k]
        d_px[k] /= num[k]

    x = []
    px = []
    for k in range(0, bins):
        if k in num:
            if num[k] > 0:
                x.append(d_x[k])
                px.append(d_px[k])

    return np.array(x), np.array(px)



# Linear regression between X and Y
# X >= Xoffset, Y >= Yoffset
def linear_regression_exponent(X, Y, Xoffset, Yoffset):

    indx1 = np.where(X >= Xoffset)[0]
    indx2 = np.where(Y >= Yoffset)[0]

    indices = np.intersect1d(indx1, indx2)
    Y = Y[indices]
    X = X[indices]

    X = np.log(X)
    Y = np.log(Y)

    Xmean = np.mean(X)
    Ymean = np.mean(Y)

    m = sum((X - Xmean) * (Y - Ymean)) / sum((X - Xmean)**2)
    c = Ymean - m*Xmean
    residulas = Y - m*X - c

    n = len(X - 2)

    num = (1. / (n-2)) * sum(residulas**2)
    den = sum((X - Xmean)**2)
    Dm = np.sqrt(num/den)

    return m, c, Dm



################################################################################
################################################################################
################################################################################


path = '../'


dataset = sys.argv[1]
delta = float(sys.argv[2])
config = dataset.lower()+'_delta'+str(delta)


f = dataset + '_ts.txt'
f_in = path + f



################################################################################
#                          BUILD AVALANCHES FROM TS


aval = {'S' : [], 'T' : []}


with open(f_in, 'r') as F:
    for line in F:
        l = line.split()
        event = [float(val) for val in l[1:]]
        results = clusters_from_TS(event, delta)

        if type(results) == dict:
            for k in aval:
                aval[k].extend(results[k])



################################################################################
#                           STATISTICAL ANALYSIS



res = {}
P = np.array(aval['S'])
L = np.array(aval['T'])

res['num_data'] = [len(P), len(L), len(L[L>0])]

x, y = log_PDF(P, 30, 1)
res['popPDF'] = [x, y]

x, y = log_PDF(L[L>0], 40, 0)
res['lifePDF'] = [x, y]

x, y = log_average(L, P, 40, 0)
res['scaling'] = [x, y]

X, Y = res['scaling'][0], res['scaling'][1]

m, c, Dm = linear_regression_exponent(X/delta, Y, 2, 1)
res['linear_regression'] = [m, c, Dm]



Smin = 2
sample = P[P >= Smin]
n = len(sample)
if n > 0:
    tau = 1. + n / sum(np.log( sample / (Smin - .5)))
    Dtau = (tau - 1.) / np.sqrt(n)


Tmin = 2*delta
sample = L[L >= Tmin]
n = len(sample)
if n > 0:
    alpha = 1. + n / sum(np.log( sample / Tmin ))
    Dalpha = (alpha - 1.) / np.sqrt(n)


res['sizeMLE'] = [tau, Dtau, n]
res['durationMLE'] = [alpha, Dalpha, n]


path = './res/' 
f = path + config + '_avalanches.npy'
np.save(f, res)










